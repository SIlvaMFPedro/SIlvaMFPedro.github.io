<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<meta name="viewport"    content="width=device-width, initial-scale=1.0">
	<meta name="description" content="Semi-Automatic Labelling for Atlascar using Adaptive Perception">
	<meta name="author"      content="Pedro Silva">
	
	<title>Semi-Automatic Labelling for Atlascar using Adaptive Perception > Datasets</title>

	<link rel="shortcut icon" href="assets/images/icon.png">
	
	<link rel="stylesheet" media="screen" href="//fonts.googleapis.com/css?family=Open+Sans:300,400,700">
	<link rel="stylesheet" href="assets/css/bootstrap.min.css">
	<link rel="stylesheet" href="assets/css/font-awesome.min.css">

	<!-- Custom styles for our template -->
	<link rel="stylesheet" href="assets/css/bootstrap-theme.css" media="screen" >
	<link rel="stylesheet" href="assets/css/main.css">

	<!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
	<!--[if lt IE 9]>
	<script src="assets/js/html5shiv.js"></script>
	<script src="assets/js/respond.min.js"></script>
	<![endif]-->
</head>

<body>
	<!-- Fixed navbar -->
	<div class="navbar navbar-inverse navbar-fixed-top headroom" >
		<div class="container">
			<div class="navbar-header">
			</div>
			<div class="navbar-collapse collapse">
				<ul class="nav navbar-nav pull-right">
					<li><a href="index.html">Home</a></li> <!-- Client area -->
					<li class="dropdown"> <!-- Specification area -->
						<a href="#" class="dropdown-toggle" data-toggle="dropdown">Specifications<b class="caret"></b></a>
						<ul class="dropdown-menu">
							<li><a href="specifications-1.html">Features, system scenarios and use cases</a></li>
							<li><a href="specifications-2.html">Requirements and tests</a></li>
							<li><a href="specifications-3.html">Architecture</a></li>
							<li class="active"><a href="specifications-4.html">Datasets</a></li>
							<li class="active"><a href="deliverables.html">Deliverables</a></li>
						</ul>
						<li><a href="about.html">About the ATLAS</a></li> <!-- Client area -->
						
						<li><a href="manual.html">Integration with CARLA</a></li> <!-- Client area -->
					</li>
				</ul>
			</div><!--/.nav-collapse -->
		</div>
	</div> 
	<!-- /.navbar -->
	<header id="head" class="secondary"></header>

	<!-- container -->
	<div class="container">

		<ol class="breadcrumb">
			<li><a href="index.html">Home</a></li>
			<li class="active">Datasets</li>
		</ol>

		<div class="row">
			
			<!-- Article main content -->
			<article class="col-sm-8 maincontent">
				<header class="page-header">
					<h1 class="page-title">ADAS Datasets - Examples of Labelled Data</h1>
				</header>

				<div class="header-inside" id="spec-4-1">
					<hr class="half-rule"/>
					This section will present some relevant work and its results in public labelled datasets used in other ADAS projects.<br><br>
					<p>&emsp;KITTI Dataset.</p><br>
					<p>&emsp;Berkeley DeepDrive Dataset</p><br>
					<p>&emsp;HumanEva II Dataset</p><br>
					<p>&emsp;ETHZ Dataset</p><br>
					<p>&emsp;EPFL Dataset</p>

					<div class="header-inside" id="spec-3-2">
						<h3>Datasets</h3>
						<hr class="half-rule"/>
						On this section we give an insight into each and every one of the datasets, explaining how they work and how they were created.</p>
						 
						<br><h4><b>KITTI Dataset</b></h4><br>
						<p>&emsp;Probably the most well-known dataset in the fields of AD is the KITTI (Karlsruhe Institute
								 of Technology 2018). The KITTI dataset was captured with a Volkswagen station wagon
								 used in mobile robotics and AD research. The KITTI benchmark suite started in
								 2012 at Karlsruhe Institute of Technology with the need to have a dataset to classify objects
								 on the streets.
								 This project has grown by increasingly adding more results with more sensors. The KITTI
								 benchmark started with the stereo, flow and odometry benchmarks and today it includes
								 standards for object tracking and more.
								 Just like ATLASCAR 2, the car used in the KITTI dataset is equipped with LIDAR
								 sensors and Point Grey Video Cameras. The dataset is used for automatic recognition and
								 tracking of vehicles and pedestrians.
								 It consists in image sequences and a text file in which, for each frame the
								 various objects in the field of view are depicted with and identification number, a label, and
								 coordinates about their position in the 2D and 3D space (Geiger et al. 2013).
								 The development kit used for the KITTI database contains C++ and MATLAB code to
								 read the sensor data and write dataset results. The data development kit used is provided
								 on the KITTI Website (Karlsruhe Institute of Technology 2018). It contains a MATLAB
								 demonstration code with C++ wrappers</p>
						<p><img src="./assets/images/Volkswaggen-KITTI.png"> <img src="./assets/images/KITTI-Deployment.png"> <img src="./assets/images/KITTI-Dataset.png"></p>
						<p><img src="./assets/images/KITTI-Tracking.png"></p>
						<p>&emsp;The demonstration script of the KITTI development kit shows how 3D boxes can be read from the dataset files and projected into the image plane of the cameras.
								 The data is processed and inserted into MATLAB structures and arrays. The KITTI database also uses the PCL to process and gather the pointclouds obtained from the LIDAR sensors.
								 The data is stored in snippets like the example seen above, in which, each line starts with the frame ID and the ID of the object being tracked. Then it is added a label
								 to classify this object. There are also flags to indicate if the object is either truncated or occluded in the image sequence.
								 The following numbers consist in the alpha (observation angle of object), the left, top, right and bottom of the 2D bounding box, the height, width and length of the 3D bounding
								 box and its XYZ coordinates. The last number consists in the 3D rotation angle in the Y axis (Boston Didi Team 2018).
								 Analyzing the snippet, a cyclist and a pedestrian can be located in frame 3 and the same cyclist and pedestrian (because they have the same object id) in the next frame with also a
								 van. The DontCare label is often shown representing an object detected that is not related to the scope of the KITTI dataset. Other information indicate where these objects are found
								 relatively to the car.</p>
						   
						</p>
						<br><h4><b>Berkeley DeepDrive Dataset</b></h4><br>
						<p>&emsp;The DeepDrive Dataset is a recent ADAS labelled dataset from the University of California, Berkeley.
								 The Dataset consists of box annotations, region annotations and detection of objects,
								 lanes and drivable areas (Yu et al. 2018). The only source data for each dataset is a video
								 captured with a camera on a vehicle. Their labelling system is a web-based tool</p>
						<p><img src="./assets/images/DeepDrive-WebTool.png"></p>
						<p>&emsp;The labelling has a semi-automatic and a manual mode. In this annotation application
								 the objects of interest are suggested with a bounding box and a category (label). The size of
								 the bounding box and the category can be edited if the system fails. The objects are detected
								 using a previously trained object detection model. The data is stored in the following snipet presented in the JSON format:</p>
						<p><img src="./assets/images/DeepDrive-Dataset.png"></p>
						<p>&emsp;For each frame there is a timestamp and a set of objects. Each object is identified with a category, an ID, attributes that indicate if the object is occluded,
							     truncated, and if the object is a traffic light which light color is on. The position is represented by the 2D bounding box position (x1, y1, x2, y2).</p>

						<br><h4><b>HumanEva II Dataset</b></h4><br>
						<p>&emsp; The HumanEva II Dataset from the Max Planck Institut Informatik (MPII) was also an interesting dataset, although it is used mainly for pedestrian detection.
							      This dataset appears with the need to represent information about detection and tracking of humans and their poses captured by a single image camera. The HumanEva dataset
								  development kit includes several MATLAB modules, each one implementing a feature. Some modules refer to the body pose, others to image stream processing, writing the dataset results,
								  and so on. Each script implements a chunk of the system that gathers the data and shapes it into MATLAB structures to be processed and to create the dataset.
								  The HumanEva dataset has information about the bounding boxes position used to track and detect pedestrian limb poses. This information is useful to know which direction the
								  person is facing from the 3D skeleton derived from the pose. The data structure in the dataset is similar to a XML file. For each frame in the image sequences there are several
								  bounding boxes with the respective coordinates (Sigal et al. 2009). From this we can generate a dataset snippet from a certain image </p>
						<p><img src="./assets/images/HumanEvaHuman.png"><img src="./assets/images/HumanEvaDataset.png"></p>
								  By looking at this dataset snippet it is easy to identify the interest points in the given frame. The files are a set of annotations called annotationList in which a path to
								  the image corresponding to the frame is given. For each image there are several bounding boxes with coordinates (x1, y1, x2, y2), a score, silhouette, articulation and viewpoint id.
						</p>
					
						
						<br><h4><b>ETHZ Dataset</b></h4><br>
						<p>&emsp;ETHZ conducted studies for detection and tracking of people on the street (Ess et al.2009). 
								 Just like the previous datasets, its creation is based in MATLAB scripts and the data is gathered and stored in MATLAB structures. 
								 The dataset is simple: for each frame there are several bounding boxes in the image.
								 This dataset is focused only in the detection and tracking of pedestrians in the image(ETHZ (Eidgenössische Technische Hochschule Zürich) 2018). 
								 In the snippet, each line is composed with a string defining a path to the image representing the actual frame, followed bytuples of four elements (x1, y1, x2, y2) representing the bounding boxes where pedestrians are
								 found in the respective frame.</p>
						<p><img src="./assets/images/ETHZ-Human.png"><img src="./assets/images/ETHZ-Dataset.png"></p>
											
						<br><h4><b>EPFL Dataset</b></h4><br>
						<p>&emsp;The EPFL designed a dataset for multiple people in a camera environment, independent of the scenario. This dataset used various synced video cameras filming the same area in
								 different angles (Biliotti, Antonini and Thiran 2015). The data from the cameras is captured and processed with MATLAB scripts and some algorithms in C++.</p>
						<p><img src="./assets/images/EPFL-Human.png"><img src="./assets/images/EPFL-Dataset.png"><img src="./assets/images/EPFL-Legend.png"></p>
						<p>&emsp;The dataset includes, for each frame, various objects identified with a number, a label, bounding box coordinates, and flags to point out
								 if the person is occluded, lost, or if the detection was automatically interpolated from the other camera’s information (EPFL (École polytechnique fédérale de Lausanne) 2018). The
								 particularity of the structure of this dataset is that, for each object, it is tracked in the image sequences individually, and only then another object is tracked and labeled.</p>
						
						<br><h4><b>Resume</b></h4><br>
						<p>&emsp;To summarize this section, after analyzing these datasets and how they are created, it is
								important to look at what is stored in the data structures . Some datasets use well known
								data structures such as XML (HumanEva) or JSON (DeepDrive), and some datasets use their
								own set of data where each line corresponds to an entry.
								To design a dataset for the ATLASCAR it is necessary to decide which construction to
								use. To simplify the complexity of the files, an adapted approach used in the KITTI dataset
								will be used, where each line is an entry for a different object.
								Analyzing the datasets, the most common and relevant information in all files is the
								position of the targets, their classification and identification. So it is fundamental for the
								ATLASCAR dataset to contain, for each frame, 2D and 3D coordinates of the target, a label,
								and an identification number.
								Regarding the labelling system, it would be interesting to implement an interface that
								suggests the user objects of interest similar to the DeepDrive annotation tool.
						</p>
	
					</div>

					

				</div>
				
			</article>
			<!-- /Article -->
			
			<!-- Sidebar -->
			<aside class="col-sm-4 sidebar sidebar-right">

				<div id="sidebar" class="row widget affix">
					<div class="col-xs-12">
						<h4>Choose the topic you want to see</h4>
						<ul class="list-unstyled list-spaces">
							<li><a href="#spec-4-1">Datasets</a><br><span class="small text-muted">This section describes the datasets used in <br>ADAS projects.</span></li>
							<li><a href="#spec-4-1">KITTI</a><br><span class="small text-muted">This section describes one of the datasets used in ADAS projects, the<br>KITTI dataset.</span></li>
							<li><a href="#spec-4-1">DeepDrive</a><br><span class="small text-muted">This section describes one of the datasets used in ADAS projects, the<br>DeepDrive dataset.</span></li>
							<li><a href="#spec-4-1">HumanEva II</a><br><span class="small text-muted">This section describes one of the datasets used in ADAS projects, the<br>HumanEva II dataset.</span></li>
							<li><a href="#spec-4-1">ETHZ</a><br><span class="small text-muted">This section describes one of the datasets used in ADAS projects, the<br>ETHZ dataset.</span></li>
							<li><a href="#spec-4-1">EPFL</a><br><span class="small text-muted">This section describes one of the datasets used in ADAS projects, the<br>EPFL dataset.</span></li>
						</ul>
					</div>
				</div>

			</aside>
			<!-- /Sidebar -->

		</div>
	</div>	<!-- /container -->
	

	<footer id="footer" class="top-space">

		<div class="footer1">
			<div class="container">
				<div class="row">
					
					<div class="col-md-6 widget">
						<h3 class="widget-title">Associated with</h3>
						<div class="widget-body">
							<!-- UA/DETI Logo -->
							<a href="https://www.ua.pt/deti/" target="_blank"><img src="assets/images/deti-ua.png" alt="UA/DETI Logo"></a>
						</div>
						<div class="widget-body">
							<!-- UA/DEM Logo -->
							<a href="https://www.ua.pt/dem/" target="_blank"><img src="assets/images/dem-ua.png" alt="UA/DEM Logo"></a>
						</div>
					</div>
					

				</div> <!-- /row of widgets -->
			</div>
		</div>

		<div class="footer2">
			<div class="container">
				<div class="row">
					
					<div class="col-md-6 widget">
						<div class="widget-body">
							<p class="simplenav">Master's Thesis - Computer and Telematics Engineering</p>
						</div>
					</div>

					<div class="col-md-6 widget">
						<div class="widget-body">
							<p class="text-right">
								Copyright &copy; 2019 - Master's Thesis(DETI/DEM). Designed by <a href="http://gettemplate.com/" rel="designer">gettemplate</a> 
							</p>
						</div>
					</div>

				</div> <!-- /row of widgets -->
			</div>
		</div>
	</footer>
		




	<!-- JavaScript libs are placed at the end of the document so the pages load faster -->
	<script src="assets/js/jquery.min.js"></script>
	<script src="assets/js/bootstrap.min.js"></script>
	<script src="assets/js/headroom.min.js"></script>
	<script src="assets/js/jQuery.headroom.min.js"></script>
	<script src="assets/js/template.js"></script>
</body>
</html>
